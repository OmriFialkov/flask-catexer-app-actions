name: ci cd with helm

on:
  push:
    paths:
      - 'charts/helm-flask/**'
      - 'app/**'
    branches:
      - main
  pull_request: # test
    paths:
      - 'charts/helm-flask/**'
      - 'app/**'
    branches:
      - main

env:
  IMAGE_NAME: crazyguy888/catexer-actions
  IMAGE_TAG: 0.0.${{ github.run_number }}

jobs:
  Build-Test:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Code
        uses: actions/checkout@v3
        with:
          fetch-depth: 3 # shallow clone - fetch last 3 commits specifically instead of last one only - for git diff

      - name: Debug GitHub SHA values
        run: |
          echo "Previous commit (before push): ${{ github.event.before }}"
          echo "Latest commit (new push): ${{ github.sha }}"
      
      - name: Debug Modified Files using Git
        run: |
          echo "Modified files between commits:"
          git diff --name-only ${{ github.event.before }} ${{ github.sha }}
         

      - name: Check If App Dir Changed
        run: | # comparing files differences between 2 last commits to find line starting with "app/"
          if git diff --name-only ${{ github.event.before }} ${{ github.sha }} | grep -q "^app/"; then
            echo "app dir changed"
            echo "APP_MODIFIED=true" >> $GITHUB_ENV
          else
            echo "app dir not changed"
            echo "APP_MODIFIED=false" >> $GITHUB_ENV
          fi

      - name: APP CI - Login to Docker Hub # prepares the environment.
        if: env.APP_MODIFIED == 'true'
        uses: docker/login-action@v2
        with:
          username: ${{ secrets.dockeruser }} # github secrets
          password: ${{ secrets.dockertoken }} # github secrets
      
      - name: APP CI - Set Environment Variables # GITHUB_ENV - just to access them quicker in current job by name only.
        if: env.APP_MODIFIED == 'true'
        run: |
            echo "FLASK_ENV=${{ vars.FLASK_ENV }}" >> $GITHUB_ENV
            echo "MYSQL_HOST=${{ vars.MYSQL_HOST }}" >> $GITHUB_ENV
            echo "MYSQL_USER=${{ vars.MYSQL_USER }}" >> $GITHUB_ENV
            echo "MYSQL_DATABASE=${{ vars.MYSQL_DATABASE }}" >> $GITHUB_ENV
            echo "PORT=${{ vars.PORT }}" >> $GITHUB_ENV
            echo "MYSQL_PASSWORD=${{ secrets.MYSQL_PASSWORD }}" >> $GITHUB_ENV
            echo "MYSQL_ROOT_PASSWORD=${{ secrets.MYSQL_ROOT_PASSWORD }}" >> $GITHUB_ENV
      
      - name: APP CI - Debug Environment Variables
        if: env.APP_MODIFIED == 'true'
        run: |
              echo "IMAGE_NAME=${IMAGE_NAME}"
              echo "IMAGE_TAG=${IMAGE_TAG}"

      - name: APP CI - Build Docker Compose Image # no cache anyway - removed --no-cache in build command.
        if: env.APP_MODIFIED == 'true'
        run: |
            docker compose build
            docker tag ${IMAGE_NAME}:${IMAGE_TAG} ${IMAGE_NAME}:latest
  
      - name: APP CI - Push Docker Image # in the end: need to do push after tests. #####
        if: env.APP_MODIFIED == 'true'
        run: |
            docker images    
            docker push ${IMAGE_NAME}:${IMAGE_TAG}
            docker push ${IMAGE_NAME}:latest

      - name: APP CI - Test - Running Project
        if: env.APP_MODIFIED == 'true'
        run: | # running without pulling nor building flask image because it is already built - docker compose reads the same image name by image context.
            ls
            docker images
            docker compose up -d
            sleep 1
            docker compose ps
      
      - name: APP CI - Run Tests # another tests. bandit etc..... #######
        if: env.APP_MODIFIED == 'true'
        run: |
            sleep 3
            docker compose logs
            echo "port being tested: ${{ env.PORT }}"
            curl -f http://localhost:${{ env.PORT }}
            echo "tests passed.."

      - name: APP CI + HELM CI - Insert Image Tag to My Helm Chart values.yaml
        if: env.APP_MODIFIED == 'true'
        run: |
          echo "updating image tag in values.yaml to ${IMAGE_TAG}"
          cd charts/helm-flask
          sed -i "s/tag: .*/tag: ${IMAGE_TAG}/g" ./values.yaml
          cat ./values.yaml

      - name: Helm CI - Install Helm
        run: |
          curl -fsSL https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
          helm version

      - name: Helm CI - Update Helm Chart Version
        run: |
          cd charts/helm-flask
          sed -i "s/version: .*/version: 0.0.${{ github.run_number }}/g" ./Chart.yaml
          echo "# this file was modified by a ci to update helm version tagging" >> ./Chart.yaml

      - name: Helm CI - Package Helm Chart
        run: |
          cd charts
          helm package helm-flask --destination ..
          cd ..
          ls -lrta
      
      - name: Helm CI - Update Helm Repo Index
        run: |
          helm repo index --url https://omrifialkov.github.io/helm-flaskgif .
          ls -lrta

      - name: Helm CI - Push Updated Helm Chart
        env:
          HELM_REPO_PAT: ${{ secrets.HELM_REPO_PAT }}
        run: |
          set -x
          git clone https://${HELM_REPO_PAT}@github.com/OmriFialkov/helm-flaskgif.git helmrepo
          cd helmrepo
          ls -lrta
          git config --global user.name "github-actions-app"
          git config --global user.email "github-actions@gmail.com"

          rm -f *.tgz && rm -f index.yaml
          cp ../*.tgz .
          cp ../index.yaml .
          ls -lrta

          git add .
          git commit -m "Updated helm chart version from APP REPO TRIGGER - Version 0.0.${{ github.run_number }}"
          git push origin main
          echo "pushed updated helm chart to helm-repo"
  CD:
    runs-on: ubuntu-latest
    needs: "Build-Test" # run only after this job is successful - if failed - won't run.
    steps:
      - name: Checkout Code
        uses: actions/checkout@v3

      - name: setup Terraform
        uses: hashicorp/setup-terraform@v2
        with: 
          terraform_wrapper: false 

      - name: Configure AWS Credentials # for s3 tf remote state - accessing s3 bucket.
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-east-1

      - name: Setup google cloud service account credentials # env configuring for terraform commands.
        run: | 
          echo '${{ secrets.GCP_SA_KEY }}' | base64 --decode > /tmp/key.json
          echo "GOOGLE_APPLICATION_CREDENTIALS=/tmp/key.json" >> $GITHUB_ENV

      - name: Install gcloud CLI # for cli commands
        uses: google-github-actions/setup-gcloud@v1

      - name: login to gcloud # using service account as a credential to login to gcloud cli.
        run: |
          gcloud auth activate-service-account --key-file=/tmp/key.json

      - name: verify gcloud login # debug.
        run: |
          set -x
          gcloud auth list
          
      - name: Terraform Init
        run: |
          cd google-k8s
          terraform init

      - name: Terraform Apply # update cluster infrastructure.
        run: |
          cd google-k8s
          terraform apply -auto-approve

      - name: Install Helm
        run: |
            curl -fsSL https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
            helm version

      - name: Add and Update Helm Repository
        run: |
            echo "$GITHUB_WORKSPACE"
            helm repo add flaskrepo https://omrifialkov.github.io/helm-flaskgif
            helm repo update
            helm search repo flaskrepo --versions

      - name: install GKE google plugin # a must install google plugin to connect to gke cluster
        run: |
            gcloud components install gke-gcloud-auth-plugin

      - name: List GKE Clusters # debug.
        run: |
            gcloud container clusters list --project ${{ secrets.GCP_PROJECT_ID }}
          
      - name: Set GKE Context to connect to cluster # runner is connecting to cluster.
        run: |
            gcloud container clusters get-credentials \
              ${{ secrets.GCP_CLUSTER_NAME }} \
              --project ${{ secrets.GCP_PROJECT_ID }} \
              --zone ${{ secrets.GCP_ZONE }}

      - name: Deploy Flask App Using Helm # Updating Release and App-Image (older with --set flag)
        run: |
          set -ex
          echo "IMAGE_TAG=${IMAGE_TAG}"
          helm upgrade --install release flaskrepo/helm-flask
          helm list

      - name: Verify Monitor Stack Is Up & Running # -q on grep is quiet mode, upgrade can be from values.yaml or published chart itself. ( this script can update both subcharts automatically - except from local chart version increment and kube-prometheus-stack remote files like default values.yaml )
        run: | # works, tested, helm list in the end to make sure whether upgrade took place or not - will go up a revision even if it did not actually upgraded something but the command succeeded, not urgent to upgrade this helm chart separately with triggers.
          set -x
          echo "checking if a conflicting monitoring release exists already in cluster by other release name and cannot be managed.."
          
          existing_release=$(helm list -A -o json | jq -r '.[] | select(.chart | startswith("monitor-stack")) | .name')
          
          if [[ -n "$existing_release" && "$existing_release" != "my-monitoring" ]]; then
            echo "monitor stack already running under release $existing_release, please rename manually to be managed by this workflow."
          
          elif [[ -n "$existing_release" && "$existing_release" == "my-monitoring" ]] || [[ -z "$existing_release" ]]; then
            echo "no conflicting monitoring release exists in cluster, will install / upgrade my-monitoring release"

            cd charts/monitor-stack/
            helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
            helm dependency update .

            if helm list -n monitor | grep -iq "my-monitoring"; then
              echo "release found, will upgrade"

              last_revision=$(helm history my-monitoring -n monitor --max 1 | awk 'NR==2 {print $1}')
              echo "last revision is $last_revision - debugging"

              if ! helm upgrade my-monitoring . -n monitor; then
                echo "upgrade may have failed - initializing rollback and exiting"
                helm rollback my-monitoring $last_revision -n monitor
                exit 1
              fi

              echo "upgrade successful"

            else
              echo "release not found, will install"


              if ! helm install my-monitoring . -n monitor; then
                echo "install may have failed - initializing cleanup and exiting"
                helm uninstall my-monitoring -n monitor
                exit 1
              fi

              echo "install successful"

            fi 

            helm status my-monitoring -n monitor
            
          fi
    

      - name: Verify Logging Stack Is Up & Running # for local subchart files - will be updated with helm upgrade only - sufficient.
        run: |
          set -x
          echo "checking if a conflicting loki-stack release exists already in cluster by other release name and cannot be managed.."

          existing_loki_release=$(helm list -A -o json | jq -r '.[] | select(.chart | startswith("loki-stack")) | .name')

          if [[ -n "$existing_loki_release" && "$existing_loki_release" != "loki" ]]; then
            echo "loki stack already running under release $existing_loki_release, please rename manually to be managed by this workflow."
          
          elif [[ -n "$existing_loki_release" && "$existing_loki_release" == "loki" ]]; then
            echo "upgrading managed-existing loki stack release.."
            helm repo add grafana https://grafana.github.io/helm-charts
            cd charts/log/
            helm upgrade loki grafana/loki-stack \
             -n log -f loki-values.yaml
            
            helm list -n log
            if helm status loki -n log | grep -iq "deployed"; then
              echo "loki upgrade successful"
            else
              echo "loki upgrade may have failed"
              exit 1
            fi

          else
            echo "loki stack release does not exist, installing under name loki.." 
            cd charts/log/ 
            helm repo add grafana https://grafana.github.io/helm-charts
            helm install loki grafana/loki-stack \
             -n log -f loki-values.yaml

            kubectl get pods -n log
            if helm status loki -n log | grep -iq "deployed"; then
              echo "loki install successful"
            else
              echo "loki install may have failed"
              exit 1
            fi
          fi

      - name: cleaning up docker hub older tags
        env:
          DOCKERTOKEN: ${{ secrets.DOCKERTOKEN }}
          DOCKER_USER: ${{ secrets.DOCKER_USER }}
          DOCKER_REPO: ${{ secrets.DOCKER_REPO }}
        run: |
          chmod +x ./cleanups/docker-clean.sh
          ./cleanups/docker-clean.sh


# fix - all ci cd in one workflow.



# NOTES:
#____________________________________________________________________________________________________

# after monitoring and logging - bonuses from here.
# --no-build in production environment
# ${IMAGE_TAG} is referenced from top level env variable IMAGE_TAG up there.
# --set flag overrides default value in values.yaml which is
# passed to flask deployment in deployment-flask.yaml so image tag is changed everytime.
# when naming workflow yaml with _ it resets its workflow run number to 1.
# with 3 helm charts - monitor, logging and flask app.
# dashboards and service monitors are manual - not in values.yaml.

# TO DO:
# ____________________________________________________________________________________________________

# HELM CHART BUILD HERE AND NOT IN ANOTHER CI
# MONITOR AND LOGGING WITH HELM UPGRADE --INSTALL - WONT DO NOTHING MOST OF
# THE TIME JUST IN CASE WE UPGRADE THE VALUES.YAML OF THE CHART - WILL IT GO UP A REVISION?
# chart umbrella additional resorces.


# FAR FUTURE:
# ____________________________________________________________________________________________________

# maybe helm chart umbrella to connect kube-prometheus-stack with additional dashboards and
# service monitors, followed by helm entirely.

# service monitors and dashboards - auto in values.yaml - check if i can.
















# deploy: # using helm cd here, 2/2/2025, azure-k8s-v2 azure ci cd with terraform deploy k8s helm.
#     runs-on: ubuntu-latest
#     needs: build-Test
#     steps:
#       - name: Checkout Code
#         uses: actions/checkout@v3

#       - name: setup Terraform
#         uses: hashicorp/setup-terraform@v2
#         with: 
#           terraform_wrapper: false 

#       - name: Configure AWS Credentials # for s3 tf remote state - accessing s3 bucket.
#         uses: aws-actions/configure-aws-credentials@v2
#         with:
#           aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
#           aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
#           aws-region: us-east-1

#       - name: azure login # to run helm in runner
#         run: |
#           set -ex
#           az login --service-principal \
#               --username ${{ secrets.AZURE_CLIENT_ID }}  \
#               --password ${{ secrets.AZURE_CLIENT_SECRET }} \
#               --tenant ${{ secrets.AZURE_TENANT_ID }}

#       - name: Terraform Init
#         env:
#           ARM_SUBSCRIPTION_ID: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
#         run: |
#           cd azure-k8s-v2
#           terraform init

#       - name: Terraform Apply # update cluster infrastructure.
#         env:
#           ARM_SUBSCRIPTION_ID: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
#         run: |
#           cd azure-k8s-v2
#           terraform apply -auto-approve

#       - name: Install Helm
#         run: |
#             curl -fsSL https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
#             helm version

#       - name: Add and Update Helm Repository
#         run: |
#             helm repo add flaskrepo https://omrifialkov.github.io/helm-flaskgif
#             helm repo update
#             helm search repo flaskrepo --versions
          
#       - name: Set AKS Context # uses az aks get-credentials internally.
#         uses: azure/aks-set-context@v3
#         with:
#           resource-group: ${{ secrets.AZURE_CLUSTER_RESOURCE_GROUP }}
#           cluster-name: ${{ secrets.AZURE_CLUSTER_NAME }}

#       - name: Deploy Flask App Using Helm, Updating Release and App-Image (--set flag)
#         run: |
#           set -ex
#           echo "IMAGE_TAG=${IMAGE_TAG}"
#           helm upgrade --install release flaskrepo/helm-flask \
#             --namespace default \
#             --set flaskApp.tag=${IMAGE_TAG}
#           helm list























  # exer 7 v2 deploy with azure k8s terraform k8s infrastructure, 27/1.
  # deploy:
  #   runs-on: ubuntu-latest
  #   needs: build-Test # ensures that the build-test job has completed successfully.
  #   steps:
  #     - name: Checkout Code
  #       uses: actions/checkout@v3

  #     - name: setup Terraform
  #       uses: hashicorp/setup-terraform@v2
  #       with: 
  #         terraform_wrapper: false 

  #     - name: Configure AWS Credentials # for s3 tf remote state - accessing s3 bucket.
  #       uses: aws-actions/configure-aws-credentials@v2
  #       with:
  #         aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
  #         aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
  #         aws-region: us-east-1

  #     - name: azure login # to run kubectl in runner
  #       run: |
  #         set -ex
  #         az login --service-principal \
  #             --username ${{ secrets.AZURE_CLIENT_ID }}  \
  #             --password ${{ secrets.AZURE_CLIENT_SECRET }} \
  #             --tenant ${{ secrets.AZURE_TENANT_ID }}

  #     - name: Terraform Init
  #       env:
  #         ARM_SUBSCRIPTION_ID: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
  #       run: |
  #         cd azure-k8s-v2
  #         terraform init

  #     - name: Terraform Apply # update cluster infrastructure.
  #       env:
  #         ARM_SUBSCRIPTION_ID: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
  #       run: |
  #         cd azure-k8s-v2
  #         terraform apply -auto-approve

  #     - name: setup kubectl
  #       uses: azure/setup-kubectl@v4
  #       with:
  #         version: 'v1.31.0'
          
  #     - name: Set AKS Context # uses az aks get-credentials internally.
  #       uses: azure/aks-set-context@v3
  #       with:
  #         resource-group: ${{ secrets.AZURE_CLUSTER_RESOURCE_GROUP }}
  #         cluster-name: ${{ secrets.AZURE_CLUSTER_NAME }}

  #     - name: deploy flask-app to k8s
  #       run: |
  #         set -ex
  #         kubectl apply -f ./k8s-config
  #         kubectl patch deployment flask-app-deployment \
  #           -p '{"spec":{"template":{"spec":{"containers":[{"name":"flask-app","image":"crazyguy888/catexer-actions:0.0.${{ github.run_number }}"}]}}}}'

      














  # disabled terraform exer 7 v1 deploy with ec2 and azure k8s temporarily 26/1. 

  # deploy:
  #   runs-on: ubuntu-latest
  #   needs: build-Test # ensures that the build-test job has completed successfully, 7.
  #   steps:
  #     - name: Checkout Code
  #       uses: actions/checkout@v3

  #     - name: setup SSH PRIVATE KEY for terraform future remote-exec
  #       run: |
  #         mkdir -p ~/.ssh
  #         echo "${{ secrets.SSH_AWS_PRIVATE_KEY }}" > ~/.ssh/my-aws-key.pem
  #         chmod 600 ~/.ssh/my-aws-key.pem

  #     - name: setup Terraform
  #       uses: hashicorp/setup-terraform@v2
  #       with: 
  #         terraform_wrapper: false

  #     - name: Terraform Init
  #       run: |
  #         cd azure-k8s
  #         terraform init 

  #     - name: Configure AWS Credentials # can be used also without built action, but it is recommended.
  #       uses: aws-actions/configure-aws-credentials@v2 # with aws configure set commands.
  #       with:
  #         aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
  #         aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
  #         aws-region: us-east-1

  #     - name: Terraform Apply
  #       env:
  #         TF_VAR_azure_client_id: ${{ secrets.AZURE_CLIENT_ID }}
  #         TF_VAR_azure_client_secret: ${{ secrets.AZURE_CLIENT_SECRET }}
  #         TF_VAR_azure_tenant_id: ${{ secrets.AZURE_TENANT_ID }}
  #       run: |
  #         cd azure-k8s
  #         terraform apply -auto-approve

  #     - name: Cleanup - Terraform Destroy
  #       env:
  #         TF_VAR_azure_client_id: ${{ secrets.AZURE_CLIENT_ID }}
  #         TF_VAR_azure_client_secret: ${{ secrets.AZURE_CLIENT_SECRET }}
  #         TF_VAR_azure_tenant_id: ${{ secrets.AZURE_TENANT_ID }}
  #       run: |
  #         cd azure-k8s
  #         terraform destroy -auto-approve

















  # disabled terraform exer 6 deploy with docker-compose temporarily 25/1.

  # deploy: # using terraform to deploy to aws ec2.
  #   runs-on: ubuntu-latest
  #   needs: build-Test # ensures that the build-test job has completed successfully.
  #   steps:
  #     - name: Checkout Code
  #       uses: actions/checkout@v3

  #     - name: Setup Terraform
  #       uses: hashicorp/setup-terraform@v2
  #       with: 
  #         terraform_wrapper: false

  #     - name: Terraform Init
  #       run: terraform init 

  #     - name: Configure AWS Credentials # can be used also without built action, but it is recommended.
  #       uses: aws-actions/configure-aws-credentials@v2 # with aws configure set commands.
  #       with:
  #         aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
  #         aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
  #         aws-region: us-east-1

  #     - name: Terraform Apply
  #       env:
  #         TF_VAR_image_tag: "0.0.${{ github.run_number }}"
  #       run: terraform apply -auto-approve


















  # disabled k8s deploy job - k8s lerning exer - to google cloud sela - temporarily for terraform testing with aws deploy 17/1.

  # deploy-k8s:
  #   runs-on: ubuntu-latest
  #   needs: test # ensures that the test job has completed successfully to proceed.
  #   steps:
  #     - name: Checkout Code
  #       uses: actions/checkout@v3

  #     - name: modify flask version in file flask-k8s.yaml
  #       run: |
  #           set -ex
  #           sed -i "s/latest/0.0.${{ github.run_number }}/g" ./k8s-config/flask-k8s.yaml
  #           cat ./k8s-config/flask-k8s.yaml

  #     - name: scp k8s yamls to vm
  #       env:
  #         SSH_PASSWORD: ${{ secrets.SSH_PASSWORD }}
  #         SSH_USER: ${{ secrets.SSH_USER }}
  #         REMOTE_HOST: ${{ secrets.REMOTE_HOST }}
  #       run: |
  #         set -ex
  #         sshpass -p $SSH_PASSWORD scp -v -o StrictHostKeyChecking=no \
  #          k8s-config/db-k8s.yaml \
  #          k8s-config/flask-k8s.yaml \
  #          k8s-config/vars-k8s.yaml \
  #          $SSH_USER@$REMOTE_HOST:/home/$SSH_USER/hey

  #     - name: ssh and apply YAMLs
  #       env:
  #         SSH_PASSWORD: ${{ secrets.SSH_PASSWORD }}
  #         SSH_USER: ${{ secrets.SSH_USER }}
  #         REMOTE_HOST: ${{ secrets.REMOTE_HOST }}
  #       run: |
  #         set -ex
  #         sshpass -p $SSH_PASSWORD ssh -v -o StrictHostKeyChecking=no \
  #          $SSH_USER@$REMOTE_HOST "cd /home/$SSH_USER/hey && kubectl apply -f ."